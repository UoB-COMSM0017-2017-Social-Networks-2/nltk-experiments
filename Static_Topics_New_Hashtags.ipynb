{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Trump',\n",
       "  'queries': ['trump',\n",
       "   '@realDonaldTrump',\n",
       "   '@POTUS',\n",
       "   'muslimban',\n",
       "   'fakenews',\n",
       "   '#americafirst',\n",
       "   '#obamagate',\n",
       "   '#trumptrain',\n",
       "   '#lockherup',\n",
       "   '#trump',\n",
       "   '#fakenews',\n",
       "   '#trumpet',\n",
       "   '#brexit',\n",
       "   '#instagram',\n",
       "   '#inspire',\n",
       "   '#love',\n",
       "   '#london',\n",
       "   '#terrorist',\n",
       "   '#londonattacks']},\n",
       " {'name': 'Russia',\n",
       "  'queries': ['#russia',\n",
       "   '#putin',\n",
       "   '#syria',\n",
       "   '#moscow',\n",
       "   '#kabul',\n",
       "   '#nato',\n",
       "   '#afganistan',\n",
       "   '#isis',\n",
       "   '#ukraine',\n",
       "   '#madvlad']},\n",
       " {'name': 'Technology',\n",
       "  'queries': ['#ArtificialIntelligence',\n",
       "   '#machinelearning',\n",
       "   '#deeplearning',\n",
       "   '@DeepMindAI',\n",
       "   '#Xboxone',\n",
       "   '#Xbox360',\n",
       "   '@Xbox',\n",
       "   '#xbox',\n",
       "   '#ps4',\n",
       "   '#ps4pro',\n",
       "   '@Playstation',\n",
       "   '@PlayStationUK',\n",
       "   '#google',\n",
       "   '#elonmusk',\n",
       "   '#spacex',\n",
       "   '#robots',\n",
       "   '#ces',\n",
       "   '#gadgets',\n",
       "   '#robotics',\n",
       "   '#robots',\n",
       "   '#bigdata',\n",
       "   '#3dprinting',\n",
       "   '#data',\n",
       "   '#datascience',\n",
       "   '#tech',\n",
       "   '#tech',\n",
       "   '#data',\n",
       "   '#technology',\n",
       "   '#ai',\n",
       "   '#google',\n",
       "   '#technation',\n",
       "   '#techno',\n",
       "   '#ps4',\n",
       "   '#machinelearning',\n",
       "   '#digital']},\n",
       " {'name': 'Sports',\n",
       "  'queries': ['MOTD',\n",
       "   'matchoftheday',\n",
       "   'UEFA',\n",
       "   '#ChampionsLeague',\n",
       "   '@Arsenal',\n",
       "   '@realmadriden',\n",
       "   'ParisSaintGermain',\n",
       "   '#Barcelona',\n",
       "   '#FCBARS',\n",
       "   '@rugbyworldcup',\n",
       "   '@EnglandRugby',\n",
       "   '@bbcrugbyunion']},\n",
       " {'name': 'NHS',\n",
       "  'queries': ['#hospital',\n",
       "   '#nhs',\n",
       "   'SaveOurNHS',\n",
       "   'OurNHS',\n",
       "   'JuniorDoctors',\n",
       "   'HuntMustGo',\n",
       "   'LoveYourNHS',\n",
       "   'NHSCrisis']},\n",
       " {'name': 'French Election',\n",
       "  'queries': ['#FrenchPresidentelection',\n",
       "   'LePen',\n",
       "   '#Hollande',\n",
       "   '#frexit',\n",
       "   '#presidentielle2017',\n",
       "   '#fillon',\n",
       "   '#marine',\n",
       "   '#leaveeu',\n",
       "   '#macron',\n",
       "   '#frenchelection']},\n",
       " {'name': 'British Politics',\n",
       "  'queries': ['brexit',\n",
       "   'remain',\n",
       "   'eumigrants',\n",
       "   'eu',\n",
       "   '#article50',\n",
       "   '#leave',\n",
       "   'proeu',\n",
       "   '#farage',\n",
       "   '#houseoflods',\n",
       "   '#PMQs',\n",
       "   '#EUref',\n",
       "   '#theresa',\n",
       "   '#brexitbritain',\n",
       "   '#johnmajor',\n",
       "   '#brexit',\n",
       "   '#brexitday',\n",
       "   '#pmqs',\n",
       "   '#article50',\n",
       "   '#london']},\n",
       " {'name': 'Television',\n",
       "  'queries': ['#eastenders',\n",
       "   '#topgear',\n",
       "   '#taboo',\n",
       "   '#ssgb',\n",
       "   '#dragonsden',\n",
       "   '#broadchurch',\n",
       "   '#thevoice',\n",
       "   '#thewalkingdead',\n",
       "   '#TWD',\n",
       "   '#24',\n",
       "   '#24legacy',\n",
       "   '#houseofcards',\n",
       "   '#HOC',\n",
       "   '#saswhodareswins',\n",
       "   '#sas',\n",
       "   '#theonlywayisessex',\n",
       "   '#TOWIE',\n",
       "   '#firstdates',\n",
       "   '#thebigbangtheory',\n",
       "   '#TGGT',\n",
       "   '#hunted',\n",
       "   '#thehunted',\n",
       "   '#fiftyshades',\n",
       "   '#50shades',\n",
       "   '#christiangrey',\n",
       "   '#grey',\n",
       "   '#towie',\n",
       "   '#eastenders']},\n",
       " {'name': 'Celebrities',\n",
       "  'queries': ['#tomhardy',\n",
       "   '#katyperry',\n",
       "   '#justinbeaber',\n",
       "   '#taylorswift',\n",
       "   '#rihanna',\n",
       "   '#justintimberlake',\n",
       "   '#kimkardashian',\n",
       "   '#harrystyles',\n",
       "   '#adele',\n",
       "   '#kanye',\n",
       "   '#emmawatson',\n",
       "   '#coldplay',\n",
       "   '#charliesheen',\n",
       "   '#simoncowell',\n",
       "   '#dicaprio',\n",
       "   '#edsheeran']},\n",
       " {'name': 'Events',\n",
       "  'queries': ['stpatricksday',\n",
       "   '#boatrace',\n",
       "   '#cambridgeoxford',\n",
       "   '#londonmarathon',\n",
       "   '#londonmara',\n",
       "   '#masters',\n",
       "   '#grandnational',\n",
       "   '#aintree']}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "import json\n",
    "import pandas as pd  \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "\n",
    "# read all sample tweets.\n",
    "with open('all_stripped_tweets.json', \"r\", encoding=\"utf-8\") as f:  \n",
    "    records=[json.loads(line) for line in f]\n",
    "    \n",
    "\n",
    "# combine relevant details into one list\n",
    "data = []\n",
    "\n",
    "for i in range(0,len(records)):\n",
    "    topic = records[i]['topic']\n",
    "    text = records[i]['text']\n",
    "    comb = [text,topic]   \n",
    "    data.append(comb)\n",
    "    \n",
    "# pass list to dataframe\n",
    "df = pd.DataFrame(data,columns=['text','topic'])\n",
    "\n",
    "    \n",
    "def get_hashtags(topic):\n",
    "    big_string = []\n",
    "    df_hash = df[df['topic']==topic]\n",
    "    for i in range(1,len(df_hash)):\n",
    "        big_string.append(df_hash.iloc[i][0])\n",
    "\n",
    "    big_string  = str(big_string)\n",
    "    toked = tknzr.tokenize(big_string)\n",
    "    hashtags = [hashtags for hashtags in toked if '#' in hashtags]\n",
    "    hash_dist = FreqDist(w.lower() for w in hashtags) \n",
    "    hash_dist_sorted = sorted(hash_dist.items(), key=itemgetter(1),reverse=True)\n",
    "    \n",
    "    # set a threshold for new hashtags - take top 10% by tweet count or those with >5 tweets\n",
    "    if (hash_dist_sorted[0][1]/10)>=5:\n",
    "        count_threshold = hash_dist_sorted[0][1]/10\n",
    "    else:\n",
    "        count_threshold = 5\n",
    "\n",
    "    #print(\"Topic: \",topic,\" - Threshold set at: \",count_threshold)#,\"\\n\\n\")\n",
    "\n",
    "    for j in range(0,10):\n",
    "        if hash_dist_sorted[j][1] >= count_threshold:\n",
    "            set_count = j\n",
    "\n",
    "    df_topic_tags = pd.DataFrame(hash_dist_sorted[0:set_count+1], columns=['Hashtag','Freq'])\n",
    "    df_topic_tags['topic'] = topic\n",
    "        \n",
    "    return df_topic_tags\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "# count tweets in example, sort and slice top 15\n",
    "df_topics = df.groupby(by=\"topic\").count()\n",
    "df_topics = df_topics.sort_values('text',ascending=False)\n",
    "df_top_topics = df_topics.iloc[0:15]\n",
    "\n",
    "all_topic_tags = pd.DataFrame()\n",
    "\n",
    "\n",
    "# step through the 15 topics and retreive the related hashtags - updating the dataframe\n",
    "for row in df_top_topics.T.iteritems():\n",
    "    topic = row[0]\n",
    "    topic_tags = get_hashtags(topic)\n",
    "    all_topic_tags = pd.concat([all_topic_tags,topic_tags])\n",
    "\n",
    "\n",
    "# bulking out static topics with previous list\n",
    "with open('ads_topics.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "\n",
    "for name in range(0,len(data)):\n",
    "    top = data[name]['name']\n",
    "    topic_list_1 = data[name][\"queries\"]\n",
    "    topic_list_2 = all_topic_tags[all_topic_tags['topic']==top]['Hashtag'].tolist()\n",
    "    data[name].update({'queries':topic_list_1+topic_list_2})\n",
    "    \n",
    "\n",
    "with open('ads_topics_NEW.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "\n",
    "    \n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
